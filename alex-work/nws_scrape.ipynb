{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWS Forecast Data\n",
    "The NWS doesn't store historical forecast data, so this notebook will just be for prototyping the scraping functions for the DAG (i.e., no preliminary scrape and data load as with the USCRN data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's save the relevant urls for each station location -- this will make it easier when we're retrieving the results for each station as part of our DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrl(row):\n",
    "  \"\"\"Construct NWS forecast url from latitude and longitude columns\"\"\"\n",
    "  url = f\"https://forecast.weather.gov/MapClick.php?lat={row['latitude']}&lon={row['longitude']}&unit=0&lg=english&FcstType=digital&menu=1\"\n",
    "  return url\n",
    "\n",
    "locations = pd.read_csv(\"data/locations.csv\")\n",
    "locations['nws_url'] = locations.apply(getUrl, axis=1)\n",
    "locations.to_csv(\"data/locations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prototyping \n",
    "Let's scrape the tabular data from one of the NWS pages and transform it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45470 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour (AKST)</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Dewpoint (°F)</th>\n",
       "      <th>Wind Chill (°F)</th>\n",
       "      <th>Surface Wind (mph)</th>\n",
       "      <th>Wind Dir</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Sky Cover (%)</th>\n",
       "      <th>Precipitation Potential (%)</th>\n",
       "      <th>Relative Humidity (%)</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Thunder</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Freezing Rain</th>\n",
       "      <th>Sleet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>02/16</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>NE</td>\n",
       "      <td></td>\n",
       "      <td>74</td>\n",
       "      <td>46</td>\n",
       "      <td>79</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>02/16</td>\n",
       "      <td>03</td>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>92</td>\n",
       "      <td>37</td>\n",
       "      <td>75</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>02/17</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>96</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Ocnl</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>02/16</td>\n",
       "      <td>06</td>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>SW</td>\n",
       "      <td></td>\n",
       "      <td>86</td>\n",
       "      <td>45</td>\n",
       "      <td>76</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/15</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NW</td>\n",
       "      <td></td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date Hour (AKST) Temperature (°F) Dewpoint (°F) Wind Chill (°F)  \\\n",
       "26  02/16          18               26            20              20   \n",
       "11  02/16          03                2            -4               2   \n",
       "42  02/17          10               23            19              14   \n",
       "14  02/16          06                2            -4               2   \n",
       "4   02/15          20                9             3               9   \n",
       "\n",
       "   Surface Wind (mph) Wind Dir Gust Sky Cover (%) Precipitation Potential (%)  \\\n",
       "26                  5       NE                 74                          46   \n",
       "11                  1        E                 92                          37   \n",
       "42                  7        S                 96                          82   \n",
       "14                  0       SW                 86                          45   \n",
       "4                   1       NW                 34                           1   \n",
       "\n",
       "   Relative Humidity (%) Rain Thunder  Snow Freezing Rain Sleet  \n",
       "26                    79   --      --   Chc            --    --  \n",
       "11                    75   --      --   Chc            --    --  \n",
       "42                    85   --      --  Ocnl            --    --  \n",
       "14                    76   --      --   Chc            --    --  \n",
       "4                     77   --      --    --            --    --  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "url = requests.get(choice(list(locations['nws_url'])))\n",
    "soup = BeautifulSoup(url.content, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all('table')\n",
    "data = tables[5].find_all(\"tr\")\n",
    "\n",
    "# Data tables are divided with colspan elements\n",
    "colspan = data[0]\n",
    "indices = [i for i, x in enumerate(data) if x == colspan]\n",
    "\n",
    "dt1 = data[indices[0]+1:indices[1]] # first 24hrs\n",
    "dt2 = data[indices[1]+1:] # next 24hrs\n",
    "\n",
    "# Combining these into one dataframe\n",
    "def getDF(l1, l2): \n",
    "\n",
    "  ls = l1.copy()\n",
    "  ls.extend(l2)\n",
    "\n",
    "  data_map = {}\n",
    "\n",
    "  for ele in ls:\n",
    "    row = [x.getText() for x in ele.find_all(\"font\")]\n",
    "\n",
    "    if row[0] not in data_map.keys(): # elements from l1\n",
    "      data_map[row[0]] = row[1:]\n",
    "    else: # elements from l2\n",
    "      data_map[row[0]].extend(row[1:])\n",
    "\n",
    "  df = pd.DataFrame(data_map)\n",
    "  df['Date'][df['Date']==\"\"] = np.NaN\n",
    "  df['Date'].ffill(inplace=True)\n",
    "\n",
    "  print(sys.getsizeof(df), sys.getsizeof(df.to_dict())) \n",
    "  return df\n",
    "\n",
    "getDF(dt1, dt2).sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the size penalty you get from representing data as a dataframe versus a simpler data type like a dictionary. We'll keep this in mind when we refactor our code next. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Old Refactor\n",
    "\n",
    "# def getDict(lists:list):  # Refactoring from getDF to process all the tables, not just two\n",
    "#   \"\"\"Unnest list of tabular data and combine into one dictionary\"\"\" \n",
    "#   ls = list(itertools.chain.from_iterable(lists)) # Unnest lists \n",
    "#   # Note: each list in lists needs to have a field inserted for station_location prior to being fed into this function\n",
    "#   data_map = {}\n",
    "#   for ele in ls:\n",
    "#     row = [x.getText() for x in ele.find_all(\"font\")]\n",
    "#     if row[0] not in data_map.keys(): # elements from l1\n",
    "#       data_map[row[0]] = row[1:]\n",
    "#     else: # elements from l2\n",
    "#       data_map[row[0]].extend(row[1:])\n",
    "\n",
    "#   data_map['Date'] = ffList(data_map['Date'])\n",
    "\n",
    "#   return data_map\n",
    "\n",
    "\n",
    "# def getForecast():\n",
    "#   \"\"\"Get data from soup objects created from NWS urls in locations.csv, then write to .csv\"\"\"\n",
    " \n",
    "#   locations = pd.read_csv(\"data/locations.csv\")\n",
    "#   loc_dict = dict(zip(locations['station_location'], locations['nws_url']))\n",
    "  \n",
    "#   ls = []\n",
    "#   for station, url in loc_dict.items():\n",
    "#     result = requests.get(url)\n",
    "#     soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "#     data = soup.find_all(\"table\")[5].find_all(\"tr\")\n",
    "\n",
    "#     # 'data' is divided into two tables by two colspan elements\n",
    "#     colspan = data[0]\n",
    "#     data = [x for  x in data if x != colspan] # add station_name here \n",
    "#     ls.append(data)\n",
    "\n",
    "#   return getDict(ls)\n",
    "# # pd.DataFrame(getForecast())  \n",
    "\n",
    "# pd.DataFrame(getForecast())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffList(ls:list) -> list:\n",
    "  \"\"\"Like ffill() from pandas, except for lists\"\"\"\n",
    "  for i in range(len(ls)):\n",
    "    if not ls[i] and i > 0:\n",
    "        ls[i] = ls[i-1]\n",
    "  return ls\n",
    "\n",
    "def getColsFromTable(table:list, location:str):\n",
    "  \"\"\"Get cols from list of <tr> elements\"\"\"\n",
    "  cols = [[ele.getText() for ele in tr.find_all(\"font\")] for tr in table] # these are rows in the table's current landscape orientation\n",
    "  location_col = ['location']\n",
    "  location_col.extend([location]*24)\n",
    "  cols.insert(1, location_col)\n",
    "  cols.insert(19, location_col) # for second table\n",
    "  return cols\n",
    "\n",
    "def getDict(col_list:list):\n",
    "  \"\"\"Get dictionary from list of columns (which are also lists)\"\"\"\n",
    "  data_map = {}\n",
    "  for col in col_list:\n",
    "    if col[0] not in data_map.keys(): # cols from first half of table\n",
    "      data_map[col[0]] = col[1:]\n",
    "    else: # cols from second half\n",
    "      data_map[col[0]].extend(col[1:])\n",
    "  data_map['Date'] = ffList(data_map['Date'])\n",
    "  return data_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nws_dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getForecast():\n",
    "  \"\"\"Get dictionary of forecast data for next 48 hours from various points in Alaska\"\"\"\n",
    "  locations = pd.read_csv(\"data/locations.csv\")\n",
    "  loc_dict = dict(zip(locations['station_location'], locations['nws_url']))\n",
    "\n",
    "  col_list = []\n",
    "  for location, url in loc_dict.items():\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "    table48 = soup.find_all(\"table\")[5].find_all(\"tr\") # list of <tr> elements from main data table (really two tables combined: one for each day in next 48h period)\n",
    "    colspan = table48[0]  # divided into two tables by two colspan elements\n",
    "    table48 = [tr for  tr in table48 if tr != colspan] # remove colspan elements\n",
    "\n",
    "    cols = getColsFromTable(table48,location)    \n",
    "    col_list.extend(cols)\n",
    "  \n",
    "  return getDict(col_list)\n",
    "\n",
    "def transformDF(myDict): \n",
    "  \"\"\"Cast dictionary from getForecast() to a dataframe, transform, and write (append) to .csv\"\"\"\n",
    "  df = pd.DataFrame(myDict)\n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "  df.replace({'':np.NaN, '--':np.NaN}, inplace=True)\n",
    "\n",
    "  ## Datetime Transformations\n",
    "  cur_year = datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour (akst)'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "\n",
    "  # reorder columns \n",
    "  cols = ['location','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "  df = df[cols]\n",
    "\n",
    "  # timestamp column: track when forecast was accessed -- DAG will run every 48 hours\n",
    "  df['fcst_date'] = datetime.now()\n",
    "\n",
    "  ## Write to CSV\n",
    "  # hdr = False  if os.path.isfile('data/forecasts.csv') else True\n",
    "  # df.to_csv('data/forecasts.csv', mode='a', index=False, header=hdr)\n",
    "\n",
    "  return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8b552aa03fd1a84f75da23b88ec62cd47e7b74ff6fa717b01373b8fb12f71a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
