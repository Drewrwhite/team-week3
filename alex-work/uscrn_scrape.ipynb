{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USCRN Data: High-Octane Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import yaml \n",
    "import re\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"sources.yaml\", \"r\") as yaml_file:\n",
    "  sources = yaml.load(yaml_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) Scrape Column Headers and Descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight). The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description). The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string). Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. See Note F. Average air temperature, in degrees C, for the entire hour. See Note F. Maximum air temperature, in degrees C, during the hour. See Note F. Minimum air temperature, in degrees C, during the hour. See Note F. Total amount of precipitation, in mm, recorded during the hour. See Note F. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. See Note G. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. See Note G. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. See Note G. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. See Note H. Average infrared surface temperature, in degrees C. See Note H. QC flag for infrared surface temperature. See Note G. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. See Note G. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. See Note G. RH average for hour, in percentage. See Note I. QC flag for RH average. See Note G. Average soil moisture at 5 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 10 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 20 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 50 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 100 cm below the surface, in m^3/m^3. See Note K. Average soil temperature at 5 cm below the surface, in degrees C. See Note K. Average soil temperature at 10 cm below the surface, in degrees C. See Note K. Average soil temperature at 20 cm below the surface, in degrees C. See Note K. Average soil temperature at 50 cm below the surface, in degrees C. See Note K. Average soil temperature at 100 cm below the surface, in degrees C. See Note K. \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_url = sources['USCRN']['headers']\n",
    "header_response = requests.get(header_url)\n",
    "header_soup = BeautifulSoup(header_response.content, \"html.parser\")\n",
    "\n",
    "columns = str(header_soup).split(\"\\n\")[1].strip(\" \").split(\" \")\n",
    "columns = list(map(lambda x: str.lower(x), columns)) # columns = [str.lower(c) for c in columns] -- faster?\n",
    "columns.insert(0,'station_location')\n",
    "\n",
    "descrip_text = str(header_soup).split(\"\\n\")[2] # raw text block containing column descriptions\n",
    "descrip_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns are quite the mess, as there is no standard separator used. We will have to work our way through it step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_split = [re.sub(r'(\\([^)]*)$', r\"\\1)\", s) for s in descrip_text.split(\"). \")] # add ')' back after splitting text on ').' \n",
    "no_notes = [re.sub(r' See Note [A-Z]\\.',\"\",s) for s in first_split] # drop any references to notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry in `no_notes` is ready. The last set of descriptions in `no_notes` can be split on `\". \"`, but the first two sets need special attention. We will pop the last set out and split it, then pop the third set out, and then address the first two sets. At that point we will recombine everything into one list while preserving the original order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_set = no_notes.pop().strip().split(\". \")\n",
    "third_set = no_notes.pop() # Note: just a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(ls:list): \n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "no_notes = [re.sub(\". Time is\", \" at\", s) for s in no_notes]\n",
    "first_second = flatten([s.split(\". \") for s in no_notes])\n",
    "\n",
    "# Finally:\n",
    "descriptions = flatten([[\"Location name for USCRN station\"], first_second, [third_set], last_set]) # Description added for \"station_location\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_info = {\n",
    "  'col_name': columns,\n",
    "  'description': descriptions, \n",
    "  'units': [\"X...(Various Lengths)\",\"XXXXX\", \"YYYYMMDD\", \"HHmm\", \"YYYYMMDD\", \"HHmm\", \"XXXXXX\", \"Decimal_degrees\", \"Decimal_degrees\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\", \"mm\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"X\", \"Celsius\", \"X\", \"Celsius\", \"X\", \"Celsius\", \"X\", \"%\", \"X\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\"]\n",
    "}\n",
    "\n",
    "header_df = pd.DataFrame(header_info)\n",
    "# header_df.to_csv(\"data/column_descriptions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.) Scrape Core Data Files (>2 million rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources[\"USCRN\"][\"index\"]\n",
    "base_soup = BeautifulSoup(requests.get(base_url).content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = base_soup.find_all(\"a\") # 'links' in this notebook will refer to <a> elements, not urls\n",
    "years = [str(x).zfill(1) for x in range(2000,2024)]\n",
    "year_links = [link for link in links if link['href'].rstrip('/') in years]\n",
    "\n",
    "file_urls = []\n",
    "for year_link in year_links: \n",
    "  year_url = base_url + year_link.get(\"href\")\n",
    "  response = requests.get(year_url) \n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  file_links = soup.find_all('a', href=re.compile(r'AK.*\\.txt'))\n",
    "  if file_links:\n",
    "    new_file_urls = [year_url + link.getText() for link in file_links]\n",
    "    file_urls.extend(new_file_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "for url in file_urls:\n",
    "  # Get location from url -- will add to BS results in next step\n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  # Get results \n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "  new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "  # Add to list\n",
    "  rows.extend(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This dataframe is huge and keeps crashing the kernel when I try to work with it. Save it as a .csv first, restart your kernel, and read it back in before continuing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/uscrn.csv\", index=False)\n",
    "df = pd.read_csv(\"data/uscrn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the original data source [README](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt):_  \n",
    "\n",
    "_\"Missing data are indicated by the lowest possible integer for a given column format, such as -9999.0 for 7-character fields with one decimal place or -99.000 for 7-character fields with three decimal places.\"_\n",
    "\n",
    "We can find these missing value indicators by getting the min of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station_location': 'Aleknagik', 'wbanno': 23583, 'utc_date': 20020809, 'utc_time': 0, 'lst_date': 20020808, 'lst_time': 0, 'crx_vn': -9.0, 'longitude': -170.21, 'latitude': 55.05, 't_calc': -9999.0, 't_hr_avg': -9999.0, 't_max': -9999.0, 't_min': -9999.0, 'p_calc': -9999.0, 'solarad': -99999, 'solarad_flag': 0, 'solarad_max': -99999, 'solarad_max_flag': 0, 'solarad_min': -99999, 'solarad_min_flag': 0, 'sur_temp_type': 'C', 'sur_temp': -9999.0, 'sur_temp_flag': 0, 'sur_temp_max': -9999.0, 'sur_temp_max_flag': 0, 'sur_temp_min': -9999.0, 'sur_temp_min_flag': 0, 'rh_hr_avg': -9999, 'rh_hr_avg_flag': 0, 'soil_moisture_5': -99.0, 'soil_moisture_10': -99.0, 'soil_moisture_20': -99.0, 'soil_moisture_50': -99.0, 'soil_moisture_100': -99.0, 'soil_temp_5': -9999.0, 'soil_temp_10': -9999.0, 'soil_temp_20': -9999.0, 'soil_temp_50': -9999.0, 'soil_temp_100': -9999.0}\n"
     ]
    }
   ],
   "source": [
    "def minMap(df):\n",
    "    min_values = {}\n",
    "    for col in df.columns:\n",
    "        mv = df[col].min()\n",
    "        min_values[col] = mv\n",
    "    return min_values\n",
    "\n",
    "print(minMap(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace these values with `NaNs`, but we need to be careful: since the source does not normally have empty records, any `NaNs` entering our pipeline on read will likely come either from errors in the data source or errors in our attempts to read from it. When writing our update DAG, before we replace any values with `NaNs` we'll need to check for `NaNs` and log an alert if any are found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([-99999,-9999], np.nan, inplace=True) # Can safely assume these are always missing values in every column they appear in\n",
    "df = df.filter(regex=\"^((?!soil).)*$\") # vast majority of soil columns have missing data\n",
    "df.replace({'crx_vn':{-9:np.nan}}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert the date and time columns to `datetime` objects and reorder our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_location', 'wbanno', 'crx_vn', 'longitude', 'latitude',\n",
       "       't_calc', 't_hr_avg', 't_max', 't_min', 'p_calc', 'solarad',\n",
       "       'solarad_flag', 'solarad_max', 'solarad_max_flag', 'solarad_min',\n",
       "       'solarad_min_flag', 'sur_temp_type', 'sur_temp', 'sur_temp_flag',\n",
       "       'sur_temp_max', 'sur_temp_max_flag', 'sur_temp_min',\n",
       "       'sur_temp_min_flag', 'rh_hr_avg', 'rh_hr_avg_flag', 'utc_datetime',\n",
       "       'lst_datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop old date and time columns\n",
    "df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns \n",
    "cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "df = df[cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's add a `date_added` column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_added_utc'] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>crx_vn</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>t_calc</th>\n",
       "      <th>t_hr_avg</th>\n",
       "      <th>t_max</th>\n",
       "      <th>...</th>\n",
       "      <th>sur_temp_type</th>\n",
       "      <th>sur_temp</th>\n",
       "      <th>sur_temp_flag</th>\n",
       "      <th>sur_temp_max</th>\n",
       "      <th>sur_temp_max_flag</th>\n",
       "      <th>sur_temp_min</th>\n",
       "      <th>sur_temp_min_flag</th>\n",
       "      <th>rh_hr_avg</th>\n",
       "      <th>rh_hr_avg_flag</th>\n",
       "      <th>date_added_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1638727</th>\n",
       "      <td>Port_Alsworth</td>\n",
       "      <td>26562.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2020-02-15 12:00:00</td>\n",
       "      <td>2020-02-15 03:00:00</td>\n",
       "      <td>-154.32</td>\n",
       "      <td>60.20</td>\n",
       "      <td>-11.3</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-10.8</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-11.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988997</th>\n",
       "      <td>Glennallen</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2022-01-30 02:00:00</td>\n",
       "      <td>2022-01-29 17:00:00</td>\n",
       "      <td>-145.50</td>\n",
       "      <td>63.03</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>-12.4</td>\n",
       "      <td>-12.1</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-16.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067823</th>\n",
       "      <td>Port_Alsworth</td>\n",
       "      <td>26562.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2017-12-13 22:00:00</td>\n",
       "      <td>2017-12-13 13:00:00</td>\n",
       "      <td>-154.32</td>\n",
       "      <td>60.20</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385979</th>\n",
       "      <td>Glennallen</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2019-04-22 08:00:00</td>\n",
       "      <td>2019-04-21 23:00:00</td>\n",
       "      <td>-145.50</td>\n",
       "      <td>63.03</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166338</th>\n",
       "      <td>Deadhorse</td>\n",
       "      <td>26565.0</td>\n",
       "      <td>2.514</td>\n",
       "      <td>2018-06-17 18:00:00</td>\n",
       "      <td>2018-06-17 09:00:00</td>\n",
       "      <td>-148.46</td>\n",
       "      <td>70.16</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_location   wbanno  crx_vn        utc_datetime  \\\n",
       "1638727    Port_Alsworth  26562.0   2.424 2020-02-15 12:00:00   \n",
       "1988997       Glennallen  56401.0   2.515 2022-01-30 02:00:00   \n",
       "1067823    Port_Alsworth  26562.0   2.424 2017-12-13 22:00:00   \n",
       "1385979       Glennallen  56401.0   2.515 2019-04-22 08:00:00   \n",
       "1166338        Deadhorse  26565.0   2.514 2018-06-17 18:00:00   \n",
       "\n",
       "               lst_datetime  longitude  latitude  t_calc  t_hr_avg  t_max  \\\n",
       "1638727 2020-02-15 03:00:00    -154.32     60.20   -11.3     -11.0  -10.8   \n",
       "1988997 2022-01-29 17:00:00    -145.50     63.03   -12.8     -12.4  -12.1   \n",
       "1067823 2017-12-13 13:00:00    -154.32     60.20     2.7       3.0    4.1   \n",
       "1385979 2019-04-21 23:00:00    -145.50     63.03    -0.5      -0.8   -0.1   \n",
       "1166338 2018-06-17 09:00:00    -148.46     70.16    -0.3      -0.5   -0.1   \n",
       "\n",
       "         ...  sur_temp_type  sur_temp  sur_temp_flag  sur_temp_max  \\\n",
       "1638727  ...              C     -11.9            0.0         -11.0   \n",
       "1988997  ...              C     -16.5            0.0         -15.8   \n",
       "1067823  ...              C       0.1            0.0           0.1   \n",
       "1385979  ...              C      -4.7            0.0          -4.5   \n",
       "1166338  ...              C       3.9            0.0           4.8   \n",
       "\n",
       "         sur_temp_max_flag  sur_temp_min  sur_temp_min_flag  rh_hr_avg  \\\n",
       "1638727                0.0         -12.8                0.0       84.0   \n",
       "1988997                0.0         -17.2                0.0       72.0   \n",
       "1067823                0.0           0.0                0.0       91.0   \n",
       "1385979                0.0          -4.9                0.0       71.0   \n",
       "1166338                0.0           3.2                0.0       88.0   \n",
       "\n",
       "        rh_hr_avg_flag             date_added_utc  \n",
       "1638727            0.0 2023-02-17 16:10:29.402040  \n",
       "1988997            0.0 2023-02-17 16:10:29.402040  \n",
       "1067823            0.0 2023-02-17 16:10:29.402040  \n",
       "1385979            0.0 2023-02-17 16:10:29.402040  \n",
       "1166338            0.0 2023-02-17 16:10:29.402040  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/uscrn.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a table for our various station locations. This will be useful when searching for the four-day forecasts in the NWS notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[['station_location', 'wbanno', 'longitude', 'latitude']].drop_duplicates()\n",
    "# locations.to_csv(\"data/locations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.) Upload Core Data to BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"data/uscrn.csv\") <-- restarted kernel again before here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>crx_vn</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>t_calc</th>\n",
       "      <th>t_hr_avg</th>\n",
       "      <th>t_max</th>\n",
       "      <th>...</th>\n",
       "      <th>sur_temp_type</th>\n",
       "      <th>sur_temp</th>\n",
       "      <th>sur_temp_flag</th>\n",
       "      <th>sur_temp_max</th>\n",
       "      <th>sur_temp_max_flag</th>\n",
       "      <th>sur_temp_min</th>\n",
       "      <th>sur_temp_min_flag</th>\n",
       "      <th>rh_hr_avg</th>\n",
       "      <th>rh_hr_avg_flag</th>\n",
       "      <th>date_added_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>2002-08-09 22:00:00.000000</td>\n",
       "      <td>2002-08-09 13:00:00.000000</td>\n",
       "      <td>-147.51</td>\n",
       "      <td>64.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>19.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>2002-08-09 23:00:00.000000</td>\n",
       "      <td>2002-08-09 14:00:00.000000</td>\n",
       "      <td>-147.51</td>\n",
       "      <td>64.97</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>2002-08-10 00:00:00.000000</td>\n",
       "      <td>2002-08-09 15:00:00.000000</td>\n",
       "      <td>-147.51</td>\n",
       "      <td>64.97</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>2002-08-10 01:00:00.000000</td>\n",
       "      <td>2002-08-09 16:00:00.000000</td>\n",
       "      <td>-147.51</td>\n",
       "      <td>64.97</td>\n",
       "      <td>12.1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>2002-08-10 02:00:00.000000</td>\n",
       "      <td>2002-08-09 17:00:00.000000</td>\n",
       "      <td>-147.51</td>\n",
       "      <td>64.97</td>\n",
       "      <td>11.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>14.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2023-02-17 16:10:29.402040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_location   wbanno  crx_vn                utc_datetime  \\\n",
       "0        Fairbanks  26494.0   1.001  2002-08-09 22:00:00.000000   \n",
       "1        Fairbanks  26494.0   1.001  2002-08-09 23:00:00.000000   \n",
       "2        Fairbanks  26494.0   1.001  2002-08-10 00:00:00.000000   \n",
       "3        Fairbanks  26494.0   1.001  2002-08-10 01:00:00.000000   \n",
       "4        Fairbanks  26494.0   1.001  2002-08-10 02:00:00.000000   \n",
       "\n",
       "                 lst_datetime  longitude  latitude  t_calc  t_hr_avg  t_max  \\\n",
       "0  2002-08-09 13:00:00.000000    -147.51     64.97     NaN       NaN    NaN   \n",
       "1  2002-08-09 14:00:00.000000    -147.51     64.97    10.0      11.1   12.0   \n",
       "2  2002-08-09 15:00:00.000000    -147.51     64.97    12.0      10.5   12.0   \n",
       "3  2002-08-09 16:00:00.000000    -147.51     64.97    12.1      11.9   12.2   \n",
       "4  2002-08-09 17:00:00.000000    -147.51     64.97    11.9      12.0   12.1   \n",
       "\n",
       "   ...  sur_temp_type  sur_temp  sur_temp_flag  sur_temp_max  \\\n",
       "0  ...              R      19.4            3.0           NaN   \n",
       "1  ...              R      14.6            0.0           NaN   \n",
       "2  ...              R      15.0            0.0           NaN   \n",
       "3  ...              R      15.4            0.0           NaN   \n",
       "4  ...              R      14.2            0.0           NaN   \n",
       "\n",
       "   sur_temp_max_flag  sur_temp_min  sur_temp_min_flag  rh_hr_avg  \\\n",
       "0                0.0           NaN                0.0        0.0   \n",
       "1                0.0           NaN                0.0        0.0   \n",
       "2                0.0           NaN                0.0        0.0   \n",
       "3                0.0           NaN                0.0        0.0   \n",
       "4                0.0           NaN                0.0        0.0   \n",
       "\n",
       "  rh_hr_avg_flag              date_added_utc  \n",
       "0            3.0  2023-02-17 16:10:29.402040  \n",
       "1            3.0  2023-02-17 16:10:29.402040  \n",
       "2            3.0  2023-02-17 16:10:29.402040  \n",
       "3            3.0  2023-02-17 16:10:29.402040  \n",
       "4            3.0  2023-02-17 16:10:29.402040  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'team-week3:alaska' successfully created.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bq mk -d --location=us-east4 team-week3:alaska"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Setting certain numeric columns (e.g. crx_vn, the flag columns) as strings will indicate that they are not meant to have arithmetic calculations done on them\n",
    "schema = [\n",
    "  bigquery.SchemaField(\"station_location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"wbanno\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"crx_vn\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"longitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"latitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"t_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"p_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_type\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"date_added_utc\", \"DATETIME\", mode=\"REQUIRED\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=8dbb2c55-bcc8-491d-8c23-e24c563dc7a6>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_path = \"/home/alex/.creds/alex-sa-tw3.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "table_id = f\"{credentials.project_id}.alaska.uscrn\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   autodetect=False,\n",
    "   schema=schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Historical weather data from USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.) DAG Task for Updating Dataset  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a neat function for reading HTML tables to dataframes (`pd.read_html`). It's not ideal for \"messier\" tabular data or for iterating through lots of HTML pages like we did earlier (iteratively creating and appending dataframes is very slow given the size of dataframe objects). But it's definitely useful for reading a single table object:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRN60H0203-202301010100.txt</td>\n",
       "      <td>2022-12-31 20:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRN60H0203-202301010200.txt</td>\n",
       "      <td>2022-12-31 21:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CRN60H0203-202301010300.txt</td>\n",
       "      <td>2022-12-31 22:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRN60H0203-202301010400.txt</td>\n",
       "      <td>2022-12-31 23:54:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRN60H0203-202301010500.txt</td>\n",
       "      <td>2023-01-01 00:49:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>CRN60H0203-202302171100.txt</td>\n",
       "      <td>2023-02-17 06:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>CRN60H0203-202302171200.txt</td>\n",
       "      <td>2023-02-17 07:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>CRN60H0203-202302171300.txt</td>\n",
       "      <td>2023-02-17 08:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>CRN60H0203-202302171400.txt</td>\n",
       "      <td>2023-02-17 09:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>CRN60H0203-202302171500.txt</td>\n",
       "      <td>2023-02-17 10:47:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name       last_modified\n",
       "0     CRN60H0203-202301010100.txt 2022-12-31 20:47:00\n",
       "1     CRN60H0203-202301010200.txt 2022-12-31 21:47:00\n",
       "2     CRN60H0203-202301010300.txt 2022-12-31 22:53:00\n",
       "3     CRN60H0203-202301010400.txt 2022-12-31 23:54:00\n",
       "4     CRN60H0203-202301010500.txt 2023-01-01 00:49:00\n",
       "...                           ...                 ...\n",
       "1138  CRN60H0203-202302171100.txt 2023-02-17 06:47:00\n",
       "1139  CRN60H0203-202302171200.txt 2023-02-17 07:47:00\n",
       "1140  CRN60H0203-202302171300.txt 2023-02-17 08:47:00\n",
       "1141  CRN60H0203-202302171400.txt 2023-02-17 09:47:00\n",
       "1142  CRN60H0203-202302171500.txt 2023-02-17 10:47:00\n",
       "\n",
       "[1143 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "updates_url = sources['USCRN']['updates'] + f\"{now.year}\"\n",
    "\n",
    "df = pd.read_html(updates_url, skiprows=[1,2])[0]\n",
    "df.drop([\"Size\", \"Description\"], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "cols = [re.sub(\" \",\"_\",str.lower(c)) for c in df.columns]\n",
    "df.columns = cols\n",
    "df['last_modified'] = pd.to_datetime(df['last_modified'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from io import StringIO\n",
    "\n",
    "with open(\"data/uscrn.csv\", 'r') as fp:\n",
    "    q = deque(fp, 1)  \n",
    "last_added = pd.read_csv(StringIO(''.join(q)), header=None).iloc[0,-1]\n",
    "last_added = datetime.strptime(last_added, \"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, last_modified]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['last_modified'] > last_added] # In actual use this won't be empty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_urls = updates_url + df['name']\n",
    "\n",
    "rows = []\n",
    "regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "for url in new_file_urls:\n",
    "  # Get location from url -- will add to BS results in next step\n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  # Get results \n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "  new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "  # Add to list\n",
    "  rows.extend(new_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8b552aa03fd1a84f75da23b88ec62cd47e7b74ff6fa717b01373b8fb12f71a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
