{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USCRN Data: High-Octane Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import yaml \n",
    "import re\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"sources.yaml\", \"r\") as yaml_file:\n",
    "  sources = yaml.load(yaml_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) Scrape Column Headers and Descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight). The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description). The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string). Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. See Note F. Average air temperature, in degrees C, for the entire hour. See Note F. Maximum air temperature, in degrees C, during the hour. See Note F. Minimum air temperature, in degrees C, during the hour. See Note F. Total amount of precipitation, in mm, recorded during the hour. See Note F. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. See Note G. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. See Note G. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. See Note G. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. See Note H. Average infrared surface temperature, in degrees C. See Note H. QC flag for infrared surface temperature. See Note G. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. See Note G. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. See Note G. RH average for hour, in percentage. See Note I. QC flag for RH average. See Note G. Average soil moisture at 5 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 10 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 20 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 50 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 100 cm below the surface, in m^3/m^3. See Note K. Average soil temperature at 5 cm below the surface, in degrees C. See Note K. Average soil temperature at 10 cm below the surface, in degrees C. See Note K. Average soil temperature at 20 cm below the surface, in degrees C. See Note K. Average soil temperature at 50 cm below the surface, in degrees C. See Note K. Average soil temperature at 100 cm below the surface, in degrees C. See Note K. \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_url = sources['USCRN']['headers']\n",
    "header_response = requests.get(header_url)\n",
    "header_soup = BeautifulSoup(header_response.content, \"html.parser\")\n",
    "\n",
    "columns = str(header_soup).split(\"\\n\")[1].strip(\" \").split(\" \")\n",
    "columns = list(map(lambda x: str.lower(x), columns)) # columns = [str.lower(c) for c in columns] -- faster?\n",
    "columns.insert(0,'station_location')\n",
    "\n",
    "descrip_text = str(header_soup).split(\"\\n\")[2] # raw text block containing column descriptions\n",
    "descrip_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns are quite the mess, as there is no standard separator used. We will have to work our way through it step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_split = [re.sub(r'(\\([^)]*)$', r\"\\1)\", s) for s in descrip_text.split(\"). \")] # add ')' back after splitting text on ').' \n",
    "no_notes = [re.sub(r' See Note [A-Z]\\.',\"\",s) for s in first_split] # drop any references to notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry in `no_notes` is ready. The last set of descriptions in `no_notes` can be split on `\". \"`, but the first two sets need special attention. We will pop the last set out and split it, then pop the third set out, and then address the first two sets. At that point we will recombine everything into one list while preserving the original order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_set = no_notes.pop().strip().split(\". \")\n",
    "third_set = no_notes.pop() # Note: just a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(ls:list): \n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "no_notes = [re.sub(\". Time is\", \" at\", s) for s in no_notes]\n",
    "first_second = flatten([s.split(\". \") for s in no_notes])\n",
    "\n",
    "# Finally:\n",
    "descriptions = flatten([[\"Location name for USCRN station\"], first_second, [third_set], last_set]) # Description added for \"station_location\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_info = {\n",
    "  'col_name': columns,\n",
    "  'description': descriptions, \n",
    "  'units': [\"X...(Various Lengths)\",\"XXXXX\", \"YYYYMMDD\", \"HHmm\", \"YYYYMMDD\", \"HHmm\", \"XXXXXX\", \"Decimal_degrees\", \"Decimal_degrees\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\", \"mm\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"X\", \"Celsius\", \"X\", \"Celsius\", \"X\", \"Celsius\", \"X\", \"%\", \"X\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\"]\n",
    "}\n",
    "\n",
    "header_df = pd.DataFrame(header_info)\n",
    "# header_df.to_csv(\"data/column_descriptions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.) Scrape Core Data Files (>2 million rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources[\"USCRN\"][\"index\"]\n",
    "base_soup = BeautifulSoup(requests.get(base_url).content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = base_soup.find_all(\"a\") # 'links' in this notebook will refer to <a> elements, not urls\n",
    "years = [str(x).zfill(1) for x in range(2000,2024)]\n",
    "year_links = [link for link in links if link['href'].rstrip('/') in years]\n",
    "\n",
    "file_urls = []\n",
    "for year_link in year_links: \n",
    "  year_url = base_url + year_link.get(\"href\")\n",
    "  response = requests.get(year_url) \n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  file_links = soup.find_all('a', href=re.compile(r'AK.*\\.txt'))\n",
    "  if file_links:\n",
    "    new_file_urls = [year_url + link.getText() for link in file_links]\n",
    "    file_urls.extend(new_file_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "for url in file_urls:\n",
    "  # Get location from url -- will add to BS results in next step\n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  # Get results \n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "  new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "  # Add to list\n",
    "  rows.extend(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This dataframe is huge and keeps crashing the kernel when I try to work with it. Save it as a .csv first, restart your kernel, and read it back in before continuing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/uscrn.csv\", index=False)\n",
    "# df = pd.read_csv(\"data/uscrn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the original data source [README](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt):_  \n",
    "\n",
    "_\"Missing data are indicated by the lowest possible integer for a given column format, such as -9999.0 for 7-character fields with one decimal place or -99.000 for 7-character fields with three decimal places.\"_\n",
    "\n",
    "We can find these missing value indicators by getting the min of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station_location': 'Aleknagik', 'wbanno': '23583', 'utc_date': '20230217', 'utc_time': '0000', 'lst_date': '20230217', 'lst_time': '1100', 'crx_vn': '2.424', 'longitude': '-131.59', 'latitude': '55.05', 't_calc': '-0.1', 't_hr_avg': '-0.3', 't_max': '-0.1', 't_min': '-0.1', 'p_calc': '-9999.0', 'solarad': '0', 'solarad_flag': '0', 'solarad_max': '0', 'solarad_max_flag': '0', 'solarad_min': '0', 'solarad_min_flag': '0', 'sur_temp_type': 'C', 'sur_temp': '-0.1', 'sur_temp_flag': '0', 'sur_temp_max': '-0.1', 'sur_temp_max_flag': '0', 'sur_temp_min': '-0.1', 'sur_temp_min_flag': '0', 'rh_hr_avg': '15', 'rh_hr_avg_flag': '0', 'soil_moisture_5': '-99.000', 'soil_moisture_10': '-99.000', 'soil_moisture_20': '-99.000', 'soil_moisture_50': '-99.000', 'soil_moisture_100': '-99.000', 'soil_temp_5': '-0.1', 'soil_temp_10': '-0.1', 'soil_temp_20': '-9999.0', 'soil_temp_50': '-9999.0', 'soil_temp_100': '-9999.0'}\n"
     ]
    }
   ],
   "source": [
    "def minMap(df):\n",
    "    min_values = {}\n",
    "    for col in df.columns:\n",
    "        mv = df[col].min()\n",
    "        min_values[col] = mv\n",
    "    return min_values\n",
    "\n",
    "print(minMap(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace these values with `NaNs`, but we need to be careful: since the source does not normally have empty records, any `NaNs` entering our pipeline on read will likely come either from errors in the data source or errors in our attempts to read from it. When writing our update DAG, before we replace any values with `NaNs` we'll need to check for `NaNs` and log an alert if any are found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([-99999,-9999], np.nan, inplace=True) # Can safely assume these are always missing values in every column they appear in\n",
    "df = df.filter(regex=\"^((?!soil).)*$\") # vast majority of soil columns have missing data\n",
    "df.replace({'crx_vn':{-9:np.nan}}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert the date and time columns to `datetime` objects and reorder our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old date and time columns\n",
    "df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns \n",
    "cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "df = df[cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's add a `date_added` column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_added_utc'] = datetime.utcnow() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>crx_vn</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>t_calc</th>\n",
       "      <th>t_hr_avg</th>\n",
       "      <th>t_max</th>\n",
       "      <th>...</th>\n",
       "      <th>sur_temp_type</th>\n",
       "      <th>sur_temp</th>\n",
       "      <th>sur_temp_flag</th>\n",
       "      <th>sur_temp_max</th>\n",
       "      <th>sur_temp_max_flag</th>\n",
       "      <th>sur_temp_min</th>\n",
       "      <th>sur_temp_min_flag</th>\n",
       "      <th>rh_hr_avg</th>\n",
       "      <th>rh_hr_avg_flag</th>\n",
       "      <th>date_added_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2040214</th>\n",
       "      <td>Metlakatla</td>\n",
       "      <td>25381.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2022-12-05 03:00:00</td>\n",
       "      <td>2022-12-04 18:00:00</td>\n",
       "      <td>-131.59</td>\n",
       "      <td>55.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989977</th>\n",
       "      <td>Glennallen</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2022-03-11 22:00:00</td>\n",
       "      <td>2022-03-11 13:00:00</td>\n",
       "      <td>-145.50</td>\n",
       "      <td>63.03</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155364</th>\n",
       "      <td>Sand_Point</td>\n",
       "      <td>25630.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2023-02-10 15:00:00</td>\n",
       "      <td>2023-02-10 06:00:00</td>\n",
       "      <td>-160.47</td>\n",
       "      <td>55.35</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>Utqiagvik</td>\n",
       "      <td>27516.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>2007-06-26 03:00:00</td>\n",
       "      <td>2007-06-25 18:00:00</td>\n",
       "      <td>-156.61</td>\n",
       "      <td>71.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492562</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>25711.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2019-06-22 07:00:00</td>\n",
       "      <td>2019-06-21 22:00:00</td>\n",
       "      <td>-170.21</td>\n",
       "      <td>57.16</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_location   wbanno  crx_vn        utc_datetime  \\\n",
       "2040214       Metlakatla  25381.0   2.424 2022-12-05 03:00:00   \n",
       "1989977       Glennallen  56401.0   2.515 2022-03-11 22:00:00   \n",
       "2155364       Sand_Point  25630.0   2.424 2023-02-10 15:00:00   \n",
       "131917         Utqiagvik  27516.0   1.301 2007-06-26 03:00:00   \n",
       "1492562         St._Paul  25711.0   2.424 2019-06-22 07:00:00   \n",
       "\n",
       "               lst_datetime  longitude  latitude  t_calc  t_hr_avg  t_max  \\\n",
       "2040214 2022-12-04 18:00:00    -131.59     55.05     0.2       0.2    0.3   \n",
       "1989977 2022-03-11 13:00:00    -145.50     63.03    -6.6      -6.4   -6.2   \n",
       "2155364 2023-02-10 06:00:00    -160.47     55.35     2.6       2.6    2.8   \n",
       "131917  2007-06-25 18:00:00    -156.61     71.32     2.2       2.4    2.8   \n",
       "1492562 2019-06-21 22:00:00    -170.21     57.16     8.5       8.7    9.0   \n",
       "\n",
       "         ...  sur_temp_type  sur_temp  sur_temp_flag  sur_temp_max  \\\n",
       "2040214  ...              C      -0.8            0.0          -0.7   \n",
       "1989977  ...              C      -4.8            0.0          -4.8   \n",
       "2155364  ...              C       2.0            0.0           2.1   \n",
       "131917   ...              R       8.7            0.0          10.2   \n",
       "1492562  ...              C       9.1            0.0           9.5   \n",
       "\n",
       "         sur_temp_max_flag  sur_temp_min  sur_temp_min_flag  rh_hr_avg  \\\n",
       "2040214                0.0          -0.9                0.0       74.0   \n",
       "1989977                0.0          -4.8                0.0       78.0   \n",
       "2155364                0.0           1.8                0.0        NaN   \n",
       "131917                 0.0           6.8                0.0        NaN   \n",
       "1492562                0.0           8.7                0.0       89.0   \n",
       "\n",
       "        rh_hr_avg_flag             date_added_utc  \n",
       "2040214            0.0 2023-02-17 22:33:04.077133  \n",
       "1989977            0.0 2023-02-17 22:33:04.077133  \n",
       "2155364            0.0 2023-02-17 22:33:04.077133  \n",
       "131917             0.0 2023-02-17 22:33:04.077133  \n",
       "1492562            0.0 2023-02-17 22:33:04.077133  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/uscrn.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make a table for our various station locations. This will be useful when searching for the four-day forecasts in the NWS notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[['station_location', 'wbanno', 'longitude', 'latitude']].drop_duplicates()\n",
    "# locations.to_csv(\"data/locations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.) Upload Core Data to BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(\"data/uscrn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'team-week3:alaska' successfully created.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bq mk -d --location=us-east4 team-week3:alaska"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Setting certain numeric columns (e.g. crx_vn, the flag columns) as strings will indicate that they are not meant to have arithmetic calculations done on them\n",
    "schema = [\n",
    "  bigquery.SchemaField(\"station_location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"wbanno\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"crx_vn\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"longitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"latitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"t_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"p_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_type\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"date_added_utc\", \"DATETIME\", mode=\"REQUIRED\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=300f1e8c-284e-42be-b54a-e8551c540ac4>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_path = \"/home/alex/.creds/alex-sa-tw3.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "table_id = f\"{credentials.project_id}.alaska.uscrn\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   autodetect=False,\n",
    "   schema=schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Historical weather data from USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.) DAG Tasks for Updating Dataset  \n",
    "\n",
    "In this section I sketch out tasks for an Airflow DAG to scrape, transform, and upload USCRN data. Note that the last three functions (`getUpdates`, `transformDF`, and `uploadBQ`) borrow heavily from the code above in sections 2 & 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Last modified</th>\n",
       "      <th>Size</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRN60H0203-202301010100.txt</td>\n",
       "      <td>2022-12-31 20:47</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRN60H0203-202301010200.txt</td>\n",
       "      <td>2022-12-31 21:47</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CRN60H0203-202301010300.txt</td>\n",
       "      <td>2022-12-31 22:53</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRN60H0203-202301010400.txt</td>\n",
       "      <td>2022-12-31 23:54</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CRN60H0203-202301010500.txt</td>\n",
       "      <td>2023-01-01 00:49</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>CRN60H0203-202302172000.txt</td>\n",
       "      <td>2023-02-17 15:47</td>\n",
       "      <td>36K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>CRN60H0203-202302172100.txt</td>\n",
       "      <td>2023-02-17 16:47</td>\n",
       "      <td>37K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>CRN60H0203-202302172200.txt</td>\n",
       "      <td>2023-02-17 17:47</td>\n",
       "      <td>36K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>CRN60H0203-202302172300.txt</td>\n",
       "      <td>2023-02-17 18:47</td>\n",
       "      <td>38K</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name     Last modified Size  Description\n",
       "0     CRN60H0203-202301010100.txt  2022-12-31 20:47  37K          NaN\n",
       "1     CRN60H0203-202301010200.txt  2022-12-31 21:47  37K          NaN\n",
       "2     CRN60H0203-202301010300.txt  2022-12-31 22:53  37K          NaN\n",
       "3     CRN60H0203-202301010400.txt  2022-12-31 23:54  37K          NaN\n",
       "4     CRN60H0203-202301010500.txt  2023-01-01 00:49  37K          NaN\n",
       "...                           ...               ...  ...          ...\n",
       "1147  CRN60H0203-202302172000.txt  2023-02-17 15:47  36K          NaN\n",
       "1148  CRN60H0203-202302172100.txt  2023-02-17 16:47  37K          NaN\n",
       "1149  CRN60H0203-202302172200.txt  2023-02-17 17:47  36K          NaN\n",
       "1150  CRN60H0203-202302172300.txt  2023-02-17 18:47  38K          NaN\n",
       "1151                          NaN               NaN  NaN          NaN\n",
       "\n",
       "[1152 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "from io import StringIO\n",
    "\n",
    "def lastAdded() -> datetime: \n",
    "  \"\"\"Reads/returns latest 'date_added_utc' value from .csv\"\"\"\n",
    "  with open(\"data/uscrn.csv\", 'r') as fp:\n",
    "    q = deque(fp, 1)  \n",
    "  last_added = pd.read_csv(StringIO(''.join(q)), header=None).iloc[0,-1]\n",
    "  last_added = datetime.strptime(last_added, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "  # Convert to EST from UTC -- 'Last modified' field in getNewFile() is given in EST\n",
    "  last_added = last_added - timedelta(hours=5)\n",
    "\n",
    "  return last_added"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a neat function for reading HTML tables to dataframes (`pd.read_html`). It wouldn't work for our earlier task because those tables were stored as loose text within a body element -- `pd.read_html` relies on explicit HTML table syntax to work. It also wouldn't be ideal for iterating through lots of HTML pages like our task called for: iteratively creating and appending dataframes is very slow given the size of dataframe objects. But it's useful for reading a single table object:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNewFileURLs(last_added:datetime) -> list: \n",
    "  \"\"\"Check/obtain updates from USCRN updates page\"\"\"\n",
    "  now = datetime.utcnow()\n",
    "  updates_url = sources['USCRN']['updates'] + str(now.year)\n",
    "\n",
    "  df = pd.read_html(updates_url, skiprows=[1,2])[0]\n",
    "  df.drop([\"Size\", \"Description\"], axis=1, inplace=True)\n",
    "  df.dropna(inplace=True)\n",
    "  df['Last modified'] = pd.to_datetime(df['Last modified'])\n",
    "\n",
    "  df = df[df['Last modified'] > last_added]\n",
    "\n",
    "  # Pushed to XCOM\n",
    "  update_range = (min(df['Last modified']), max(df['Last modified'])) # Push to XCOM\n",
    "\n",
    "  new_file_urls = list(updates_url + \"/\" + df['Name'])\n",
    "\n",
    "  return new_file_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getUpdates(new_file_urls:list) -> dict: \n",
    "  \"\"\"Scrape data from list of new urls, store and return as list of lists\"\"\"\n",
    "\n",
    "  locations = pd.read_csv(\"data/locations.csv\")[['station_location', 'wbanno']]\n",
    "  locations['wbanno'] = locations['wbanno'].astype(int).astype(str)\n",
    "  wbs = set(locations['wbanno'])\n",
    "\n",
    "  rows = []\n",
    "  for url in new_file_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    soup_lines = str(soup).strip().split(\"\\n\")[3:]\n",
    "    ak_rows = [re.split('\\s+', line) for line in soup_lines if line[0:5] in wbs]\n",
    "    rows.extend(ak_rows)\n",
    "\n",
    "  return rows\n",
    "\n",
    "\n",
    "def transformDF(rows:list): \n",
    "  \"\"\"Read rows from getUpdates(), cast to dataframe, transform, write to csv\"\"\"\n",
    "  \n",
    "  # Get column headers \n",
    "  columns = list(pd.read_csv(\"data/column_descriptions.csv\")['col_name'])\n",
    "\n",
    "  # Get locations\n",
    "  locations = pd.read_csv(\"data/locations.csv\")[['station_location', 'wbanno']]\n",
    "  locations['wbanno'] = locations['wbanno'].astype(int).astype(str) \n",
    "  locations.set_index(\"wbanno\", inplace=True)\n",
    "\n",
    "  # Create dataframe\n",
    "  df = pd.DataFrame(rows, columns=columns[1:])\n",
    "\n",
    "  # Merge locations\n",
    "  df = df.merge(locations, how=\"left\", left_on=\"wbanno\", right_index=True)\n",
    "\n",
    "  # Reorder columns \n",
    "  columns = ['station_location'] + list(df.columns)[:-1]\n",
    "  df = df[columns]\n",
    "\n",
    "  # Change datatypes\n",
    "  df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "  # Replace missing value designators with NaN\n",
    "  df.replace([-99999,-9999], np.nan, inplace=True) \n",
    "  df.replace({'crx_vn':{-9:np.nan}}, inplace=True)\n",
    "  df = df.filter(regex=\"^((?!soil).)*$\") # almost all missing values\n",
    "\n",
    "  # Create datetime columns\n",
    "  df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "  df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "  # Drop old date and time columns \n",
    "  df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "\n",
    "  # Reorder columns \n",
    "  cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "  df = df[cols]\n",
    "\n",
    "  # Add date-added column (utc)\n",
    "  df['date_added_utc'] = datetime.utcnow() \n",
    "\n",
    "  # Write to .csv\n",
    "  # Pull `update_range` from XCOM (created by 'getNewFileUrls())\n",
    "  df.to_csv(f\"data/updates/upd-{update_range[0]}-{update_range[1]}.csv\") # \n",
    "\n",
    "def uploadBQ():\n",
    "  \"\"\"Upload latest update file to BigQuery\"\"\"\n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.) **TO-DO**: _Remove Temporary Section_ \n",
    "This section is for testing the above functions for the DAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 2, 17, 17, 33, 4, 77133)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/uscrn.csv\", 'r') as fp:\n",
    "  q = deque(fp, 1)  \n",
    "last_added = pd.read_csv(StringIO(''.join(q)), header=None).iloc[0,-1]\n",
    "last_added = datetime.strptime(last_added, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "# Convert to EST from UTC -- 'Last modified' field in getNewFile() is given in EST\n",
    "last_added = last_added - timedelta(hours=5)\n",
    "last_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/updates/2023/CRN60H0203-202302172200.txt',\n",
       " 'https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/updates/2023/CRN60H0203-202302172300.txt',\n",
       " 'https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/updates/2023/CRN60H0203-202302180000.txt',\n",
       " 'https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/updates/2023/CRN60H0203-202302180100.txt']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def getNewFileURLs(last_added:datetime) -> list: \n",
    "  # \"\"\"Check/obtain updates from USCRN updates page\"\"\"\n",
    "now = datetime.utcnow()\n",
    "updates_url = sources['USCRN']['updates'] + f\"{now.year}\"\n",
    "\n",
    "df = pd.read_html(updates_url, skiprows=[1,2])[0]\n",
    "df.drop([\"Size\", \"Description\"], axis=1, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['Last modified'] = pd.to_datetime(df['Last modified']) \n",
    "\n",
    "df = df[df['Last modified'] > last_added]\n",
    "\n",
    "new_file_urls = list(updates_url + \"/\" + df['Name'])\n",
    "new_file_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['23583', '20230217', '2100', '20230217', '1200', '2.514', '-158.61', '59.28', '2.2', '2.1', '2.3', '1.7', '0.0', '161', '0', '331', '0', '38', '0', 'C', '-1.3', '0', '0.0', '0', '-2.7', '0', '70', '0', '-99.000', '-99.000', '-99.000', '-99.000', '-99.000', '-9999.0', '-9999.0', '-9999.0', '-9999.0', '-9999.0']\n"
     ]
    }
   ],
   "source": [
    "# def getUpdates(new_file_urls:list) -> dict: \n",
    "# \"\"\"Scrape data from list of new urls, store and return as list of lists\"\"\"\n",
    "\n",
    "locations = pd.read_csv(\"data/locations.csv\")[['station_location', 'wbanno']]\n",
    "locations['wbanno'] = locations['wbanno'].astype(int).astype(str)\n",
    "wbs = set(locations['wbanno'])\n",
    "\n",
    "rows = []\n",
    "for url in new_file_urls:\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = str(soup).strip().split(\"\\n\")[3:]\n",
    "  ak_rows = [re.split('\\s+', line) for line in soup_lines if line[0:5] in wbs]\n",
    "  rows.extend(ak_rows)\n",
    "print(rows[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transformDF(rows:list): \n",
    "#   \"\"\"Read rows from getUpdates(), cast to dataframe, transform, write to csv\"\"\"\n",
    "\n",
    "# Get column headers \n",
    "columns = list(pd.read_csv(\"data/column_descriptions.csv\")['col_name'])\n",
    "\n",
    "# Get locations\n",
    "locations = pd.read_csv(\"data/locations.csv\")[['station_location', 'wbanno']]\n",
    "locations['wbanno'] = locations['wbanno'].astype(int).astype(str) \n",
    "locations.set_index(\"wbanno\", inplace=True)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(rows, columns=columns[1:])\n",
    "\n",
    "# Merge locations\n",
    "df = df.merge(locations, how=\"left\", left_on=\"wbanno\", right_index=True)\n",
    "\n",
    "# Reorder columns \n",
    "columns = ['station_location'] + list(df.columns)[:-1]\n",
    "df = df[columns]\n",
    "# Change datatypes\n",
    "df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "# Replace missing value designators with NaN\n",
    "df.replace([-99999,-9999], np.nan, inplace=True) \n",
    "df.replace({'crx_vn':{-9:np.nan}}, inplace=True)\n",
    "df = df.filter(regex=\"^((?!soil).)*$\") # almost all missing values\n",
    "\n",
    "# Create datetime columns\n",
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "# Drop old date and time columns \n",
    "df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns \n",
    "cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "df = df[cols]\n",
    "\n",
    "# Add date-added column (utc)\n",
    "df['date_added_utc'] = datetime.utcnow() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16 (default, Dec  7 2022, 01:12:33) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8b552aa03fd1a84f75da23b88ec62cd47e7b74ff6fa717b01373b8fb12f71a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
